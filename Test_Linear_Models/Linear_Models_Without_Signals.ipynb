{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caractérisation de l'overfitting de différents modèles linéaires dans le cadre d'un modèle sans signaux\n",
    "\n",
    "Dans ce notebook, à partir d'une certaine matrice de rendements, nous allons utiliser différents modèles linéaires afin de construire des portefeuilles les plus efficaces possible. \n",
    "\n",
    "- Les portefeuilles sont, en fait, des vecteurs dont les composantes sont la pondération de chacun des actifs dans notre investissement. \n",
    "- L'efficacité du portefeuille est mesurée à partir de la donnée du Sharpe Ratio (rendement / écart-type)\n",
    "- La quantification de l'overfitting sera calculée à partir de la performance relative du portefeuille out-sample par rapport à sa performance in-sample. On utilisera aussi le rapport de la fonction de loss quadratique du modèle out-sample sur celle in-sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "#Import des packages utiles \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import des fonctions \"outils\"\n",
    "from Test_ToolsFunctions import rdt_matrix_generator\n",
    "\n",
    "#Import des modèles linéaires\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoCV #Cross-validation intégrée\n",
    "from sklearn.linear_model import RidgeCV #Cross-validation intégrée\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Lars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désormais, nous allons nous placer dans le cas 1) où tous les actifs suivent la même loi. Nous allons tester et afficher le graphique de l'évolution des sharpes ratios in et out sample en fonction de la variation de tous ces paramètres. \n",
    "Nous afficherons l'évolution de la fonction de perte quadratique in & out sample. \n",
    "\n",
    "Les paramètres qui varieront : \n",
    "- Nombre d'actifs\n",
    "- Nombre de dates dans l'échantillon d'entraînement\n",
    "- Espérance des rendements (constante pour tous)\n",
    "- Variance des rendements (constante)\n",
    "- Corrélation entre les actifs\n",
    "- Ratio Nombre de lignes / Nombre de colonnes dans la matrice des rendements (combinaison du nombre d'actifs et du nombre de dates de train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthodologie\n",
    "\n",
    "Désormais, nous allons utiliser une nouvelle approche afin de limiter le temps de calcul. A chaque pas, c'est-à-dire à chaque fois que l'on fait varier un paramètre du problème et que l'on fit un nouveau modèle, nous procédons de la sorte : \n",
    "\n",
    "- 1) Par Cross-Validation, on va tuner le paramètre du modèle en question (par exemple la valeur de la pénalisation) afin qu'il soit le moins overfitté possible. Ainsi, on pourra comparer les performances optimales out-sample de chacun des modèles. Parfois cette tâche pourra être facilitée par les modèles fournis par Sklearn qui peuvent être capable de choisir directement le paramètre optimal du modèle (Ex: LassoCV ou RidgeCV)\n",
    "- 2) De la même façon que pour une K-fold Cross-Validation, on génère K matrices de rendements correspondant aux nouveaux paramètres et l'on y fit K modèles avec la méthode testée. \n",
    "\n",
    "Remarque : Dans un but de simplification computationnelle, dès la 1ère itération, on va fixer les vecteurs d'aléas relatifs à notre matrice de rendements. C'est-à-dire qu'il suffira à chaque itération de réutiliser ces K vecteurs gardés en mémoire pour éviter d'avoir à les regénérer et avoir une courbe finale plus lisse. (On considère que le temps d'exécution de la génération des matrices aléatoires est grandement expliqué par cette étape mais à vérifier). Par exemple, si le paramètre qui varie est l'espérance ou la matrice variance-covariance de notre vecteur aléatoire, on garde le vecteur d'aléa initial et quelques variations dans le code seront suffisante pour générer le nouveau vecteur adéquat. Enfin, lorsque l'on souhaitera faire varier la taille de cette matrice de rendements, on génèrera des matrices de taille maximales et à chaque itération on utilisera la sous-matrice qui nous intéresse (la grande matrice qui se découvre petit à petit est ce un problème -> introduction de biais dans le modèle au sens ou la première ligne/colonne sera toujours utilisée)\n",
    "\n",
    "- 3) Ensuite, il suffira de tester chacun des K modèles sur les (K-1) datasets sur lesquels il n'a pas été entraîné (-> gain computationnel de ne pas générer de nouveaux datasets de tests + constat suivant : faire 1000 simus de tests de 1 an puis la moyenne des sharpes équivalent à regarde le sharpe moyen sur un échantillon test de 1000 ans ? Si non, est ce que cette approche reste quand même valide ? Si oui, on obtient le même résultat que l'approche précédent avec les datatests de 1an & si non, pas grave on aura des échantillons de tests qui seront de même taille que le train ou alors on split chacun des K-1 en datasets de taille 1 an). Ainsi, on obtient la moyenne de la performance out-sample de chaque modèles et on le re-moyennise afin de tracer la courbe de la performance MOYENNE du type de modèle testé.   \n",
    "\n",
    "- 4) On obtient alors un graphe montrant l'évolution de la performance moyenne in & out sample du type de modèle testé selon la variation du paramètre d'intérêt. On finir par tracer l'évolution du rapport Sharpe Out sur Sharpe In dont on pense qu'il donne la tendance de l'overfitting et de plus, la valeur intrinsèque de ce rapport nous intéresse car elle indique de \"combien\" baisse le sharpe en passant du IN au OUT (vérifier si on a aucun biais concernant cette valeur absolue et qu'elle est bien comparable entre les modèles). \n",
    "\n",
    "- 5) A chaque étape garder trace de la quadratic loss in & out sample et la comparer aussi ???? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation du nombre d'actifs\n",
    "\n",
    "Ici, nous allons faire varier le nombre de colonnes de la matrice (nb d'actifs). \n",
    "Pour ce qui est de loi des actifs on garde celle de l'exemple initial -> à savoir tous suivent la même loi. Seulement dans un second temps, on testera avec des vecteurs d'actifs qui ont des lois différentes (pas fait encore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Régression Simple (Benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
