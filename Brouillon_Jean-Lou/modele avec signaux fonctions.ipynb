{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\jean-\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\jean-\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jean-\\anaconda3\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jean-\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Accès refusé: 'C:\\\\Users\\\\jean-\\\\anaconda3\\\\Lib\\\\site-packages\\\\~klearn\\\\linear_model\\\\_cd_fast.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\jean-\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jean-\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\jean-\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jean-\\anaconda3\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Can't uninstall 'joblib'. No files were found to uninstall.\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Can't uninstall 'scikit-learn'. No files were found to uninstall.\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.1.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing requirements for scikit-learn: [Errno 2] No such file or directory: 'c:\\\\users\\\\jean-\\\\anaconda3\\\\lib\\\\site-packages\\\\scikit_learn-0.24.2.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for joblib: [Errno 2] No such file or directory: 'c:\\\\users\\\\jean-\\\\anaconda3\\\\lib\\\\site-packages\\\\joblib-1.1.0.dist-info\\\\METADATA'\n",
      "    WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\jean-\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jean-\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade scikit-learn\n",
    "%pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parametres initiaux du notebook du P \n",
    "\n",
    "nb_signals = 100\n",
    "nb_dates_in_sample = 1300\n",
    "nb_dates_out_sample = 260\n",
    "nb_assets = 80\n",
    "\n",
    "vol_assets = 0.2/16\n",
    "correl_assets = 0.8\n",
    "\n",
    "# vol_signals = 1.0\n",
    "# correl_signals = 0.0\n",
    "\n",
    "vol_signals = 0.2/16\n",
    "correl_signals = 0.8\n",
    "\n",
    "signal_to_noise = 0.001   # la relation entre rendements X et signaux Y n'est pas parfaitement proportionnelle à cause de ça ?\n",
    "\n",
    "rank_betas = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction qui renvoie un dataframe avec la moyenne des sharpes prédits out sample, la moyenne des sharpes optimaux théoriques, l'écart entre les 2 moyennes, et les valeurs des paramètres utilisés en input \n",
    "-> à voir si ça vaut le coup de ne pas mettre tous les paramètres dans le dataframe renvoyé par la fonction pour accélérer les calculs ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas):\n",
    "    \n",
    "    liste_sharpes_predit = []\n",
    "    liste_sharpes_optimaux_os=[]\n",
    "    écart_sharpe_moyen=[]\n",
    "    \n",
    "    cov_assets = vol_assets**2 * pd.DataFrame(correl_assets * np.ones((nb_assets,nb_assets)) + (1-correl_assets)*np.eye(nb_assets))\n",
    "    cov_signals = vol_signals**2 * pd.DataFrame(correl_signals * np.ones((nb_signals,nb_signals)) + (1-correl_signals)*np.eye(nb_signals))\n",
    "\n",
    "    for i in range(nb_simus): \n",
    "        #partie generation des matrices de signaux et de rendements, d'entrainement et de test \n",
    "        \n",
    "        betas = pd.DataFrame(np.random.randn(nb_signals,nb_assets)) \n",
    "        betas *= np.sqrt(signal_to_noise * np.trace(cov_assets) / np.trace(betas.T @ cov_signals @ betas))\n",
    "        u,d,v = np.linalg.svd(betas)\n",
    "        betas = pd.concat([pd.DataFrame(d[k] * np.outer(u[:,k],v[k,:]),index=betas.index,columns=betas.columns) for k in range(rank_betas)],keys=range(rank_betas)).groupby(level=1).sum()\n",
    "        betas *= np.sqrt(signal_to_noise * np.trace(cov_assets) / np.trace(betas.T @ cov_signals @ betas))\n",
    "    \n",
    "        cov_noise = cov_assets - betas.T @ cov_signals @ betas\n",
    "        d,P = np.linalg.eigh(cov_noise)\n",
    "        cov_noise = pd.DataFrame(P @ np.diag(np.maximum(d,1e-10)) @ P.T)\n",
    "\n",
    "        sqrt_cov_assets = np.linalg.cholesky(cov_assets)\n",
    "        sqrt_cov_signals = np.linalg.cholesky(cov_signals)\n",
    "        sqrt_cov_noise = np.linalg.cholesky(cov_noise)\n",
    "    \n",
    "        signals = pd.DataFrame(np.random.randn(nb_dates_in_sample,nb_signals) @ sqrt_cov_signals.T)\n",
    "        noise = pd.DataFrame(np.random.randn(nb_dates_in_sample,nb_assets) @ sqrt_cov_noise.T)\n",
    "        assets = signals @ betas+noise\n",
    "    \n",
    "        signals_os = pd.DataFrame(np.random.randn(nb_dates_out_sample,nb_signals) @ sqrt_cov_signals.T)\n",
    "        noise_os = pd.DataFrame(np.random.randn(nb_dates_out_sample,nb_assets) @ sqrt_cov_noise.T)\n",
    "        assets_os = signals_os @ betas+noise_os\n",
    "\n",
    "    \n",
    "    \n",
    "        #partie entrainement et test du modele \n",
    "        modele.fit(signals,assets)\n",
    "        prediction = modele.predict(signals_os) #ce truc est le vecteur des pondérations w=Y'f(Y)\n",
    "        pnl_predit=(prediction * assets_os).sum(axis=1)   #Xw = le vecteur des pnls, on obtient pnl quand on somme les rendements quotidiens \n",
    "        liste_sharpes_predit.append(pnl_predit.mean()/pnl_predit.std()*16)\n",
    "    \n",
    "        #question : a quoi comparer pnl_predit et sharpe_predit? \n",
    "        #à pnl optimal out sample ou in sample ? ie avec Xtrain ou avec Xtest ?\n",
    "        pnl_optimal_os = ((signals_os @ betas)*assets_os).sum(axis=1)\n",
    "        liste_sharpes_optimaux_os.append(pnl_optimal_os.mean()/pnl_optimal_os.std()*16)\n",
    "        \n",
    "        écart_sharpe_moyen.append(pnl_optimal_os.mean()/pnl_optimal_os.std()*16 - pnl_predit.mean()/pnl_predit.std()*16)\n",
    "       \n",
    "    \n",
    "    dic = {'nb_simus': nb_simus,\n",
    "       'moyenne sharpes prédits': np.mean(liste_sharpes_predit),\n",
    "       'moyenne sharpe optimaux': np.mean(liste_sharpes_optimaux_os),\n",
    "       'écart moyen sharpe': np.mean(écart_sharpe_moyen),\n",
    "     }\n",
    "\n",
    "    resultat = pd.DataFrame([dic])\n",
    "\n",
    "    \n",
    "    return resultat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #autres trucs qui étaient dans le dataframe en output à la base mais qu'on retire pour accélerer calculs\n",
    "'nb_signals': nb_signals,\n",
    "       'nb_dates_in_sample': nb_dates_in_sample,\n",
    "       'nb_dates_out_sample': nb_dates_out_sample,\n",
    "       'nb_assets': nb_assets,\n",
    "       'vol_assets': vol_assets,\n",
    "       'correl_assets': correl_assets,\n",
    "       'vol_signals': vol_signals,\n",
    "       'correl_signals': correl_signals,\n",
    "       'signal_to_noise': signal_to_noise,\n",
    "       'rank_betas': rank_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "solve() got an unexpected keyword argument 'sym_pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6376/4009385892.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m x=prediction_sharpe(modele=Ridge(fit_intercept=False), \n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mnb_simus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mnb_signals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mnb_dates_in_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mnb_dates_out_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m260\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6376/69890746.py\u001b[0m in \u001b[0;36mprediction_sharpe\u001b[1;34m(modele, nb_simus, nb_signals, nb_dates_in_sample, nb_dates_out_sample, nb_assets, vol_assets, correl_assets, vol_signals, correl_signals, signal_to_noise, rank_betas)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m#partie entrainement et test du modele\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mmodele\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0massets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodele\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignals_os\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#ce truc est le vecteur des pondérations w=Y'f(Y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mpnl_predit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0massets_os\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#Xw = le vecteur des pnls, on obtient pnl quand on somme les rendements quotidiens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jean-\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m                 \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                 \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jean-\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m     \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jean-\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py\u001b[0m in \u001b[0;36m_ridge_regression\u001b[1;34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, random_state, return_n_iter, return_intercept, X_scale, X_offset, check_input)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[1;33m-\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m \u001b[0muses\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mBFGS\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mB\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0mimplemented\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m           \u001b[0;31m`\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mIt\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0monly\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m           \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jean-\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py\u001b[0m in \u001b[0;36m_solve_cholesky\u001b[1;34m(X, y, alpha)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mX_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0msample_weight_sqrt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m ):\n\u001b[0m\u001b[0;32m    148\u001b[0m     \"\"\"Solve Ridge regression via LSQR.\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: solve() got an unexpected keyword argument 'sym_pos'"
     ]
    }
   ],
   "source": [
    "x=prediction_sharpe(modele=Ridge(fit_intercept=False), \n",
    "                    nb_simus=10,\n",
    "                    nb_signals=100,\n",
    "                    nb_dates_in_sample=1300,\n",
    "                    nb_dates_out_sample=260,\n",
    "                    nb_assets=80,\n",
    "                    vol_assets=0.2/16,\n",
    "                    correl_assets=0.8,\n",
    "                    vol_signals=0.2/16,\n",
    "                    correl_signals=0.8,\n",
    "                    signal_to_noise=0.001,\n",
    "                    rank_betas=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remarque: le ridge ne fonctionne pas sur vs code (pb de compatibilité de python??) il faut faire tourner sur ssp cloud pour obtenir des résultats..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_simus</th>\n",
       "      <th>moyenne sharpes prédits</th>\n",
       "      <th>moyenne sharpe optimaux</th>\n",
       "      <th>écart moyen sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.182454</td>\n",
       "      <td>5.14349</td>\n",
       "      <td>5.325945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nb_simus  moyenne sharpes prédits  moyenne sharpe optimaux  \\\n",
       "0        10                -0.182454                  5.14349   \n",
       "\n",
       "   écart moyen sharpe  \n",
       "0            5.325945  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jolis_graphiques(modele, parametre_en_abscisse, min_abscisse, max_abscisse,pas, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas):\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    abscisse = np.arange(min_abscisse,max_abscisse,pas) #abscisse\n",
    "    sharpe_os = []\n",
    "    sharpe_theo = []\n",
    "    ecart_sharpe = []\n",
    "\n",
    "    \n",
    "    if parametre_en_abscisse == 'nombre_signaux' : \n",
    "        \n",
    "        for i in abscisse : \n",
    "\n",
    "            sharpe_os.append(prediction_sharpe(modele, nb_simus, i,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,1])\n",
    "            sharpe_theo.append(prediction_sharpe(modele, nb_simus,i,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,2])\n",
    "            ecart_sharpe.append(prediction_sharpe(modele, nb_simus,i,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,3])\n",
    "\n",
    "\n",
    "    elif parametre_en_abscisse == 'nombre_dates_in_sample' : \n",
    "\n",
    "        for i in abscisse:\n",
    "\n",
    "            print(i)\n",
    "            sharpe_os.append(prediction_sharpe(modele, nb_simus, nb_signals,i,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,1])\n",
    "            sharpe_theo.append(prediction_sharpe(modele, nb_simus,nb_signals,i,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,2])\n",
    "            ecart_sharpe.append(prediction_sharpe(modele, nb_simus,nb_signals,i,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,3])\n",
    "\n",
    "\n",
    "    elif parametre_en_abscisse == 'nombre_dates_out_sample' : \n",
    "\n",
    "        for i in abscisse:\n",
    "\n",
    "            sharpe_os.append(prediction_sharpe(modele, nb_simus, nb_signals,nb_dates_in_sample,i,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,1])\n",
    "            sharpe_theo.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,i,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,2])\n",
    "            ecart_sharpe.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,i,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,3])\n",
    "\n",
    "\n",
    "    elif parametre_en_abscisse == 'nombre_actifs' : \n",
    "\n",
    "        for i in abscisse:\n",
    "\n",
    "            sharpe_os.append(prediction_sharpe(modele, nb_simus, nb_signals,nb_dates_in_sample,nb_dates_out_sample,i,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,1])\n",
    "            sharpe_theo.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,i,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,2])\n",
    "            ecart_sharpe.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,i,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,3])\n",
    "\n",
    "\n",
    "    elif parametre_en_abscisse == 'vol_actifs' : \n",
    "\n",
    "        for i in abscisse:\n",
    "\n",
    "            sharpe_os.append(prediction_sharpe(modele, nb_simus, nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,i,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,1])\n",
    "            sharpe_theo.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,i,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,2])\n",
    "            ecart_sharpe.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,i,\n",
    "                      correl_assets,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,3])\n",
    "\n",
    "\n",
    "    elif parametre_en_abscisse == 'correl_actifs' : \n",
    "\n",
    "        for i in abscisse:\n",
    "\n",
    "            sharpe_os.append(prediction_sharpe(modele, nb_simus, nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      i,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,1])\n",
    "            sharpe_theo.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      i,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,2])\n",
    "            ecart_sharpe.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      i,vol_signals,correl_signals,signal_to_noise,rank_betas).iloc[0,3])\n",
    "\n",
    "\n",
    "\n",
    "    elif parametre_en_abscisse == 'vol_signaux' : \n",
    "\n",
    "        for i in abscisse:\n",
    "\n",
    "            sharpe_os.append(prediction_sharpe(modele, nb_simus, nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,i,correl_signals,signal_to_noise,rank_betas).iloc[0,1])\n",
    "            sharpe_theo.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,i,correl_signals,signal_to_noise,rank_betas).iloc[0,2])\n",
    "            ecart_sharpe.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,i,correl_signals,signal_to_noise,rank_betas).iloc[0,3])\n",
    "\n",
    "\n",
    "\n",
    "    elif parametre_en_abscisse == 'correl_signaux' : \n",
    "\n",
    "        for i in abscisse:\n",
    "\n",
    "            sharpe_os.append(prediction_sharpe(modele, nb_simus, nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,i,signal_to_noise,rank_betas).iloc[0,1])\n",
    "            sharpe_theo.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,i,signal_to_noise,rank_betas).iloc[0,2])\n",
    "            ecart_sharpe.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,i,signal_to_noise,rank_betas).iloc[0,3])\n",
    "\n",
    "\n",
    "\n",
    "    elif parametre_en_abscisse == 'signal_sur_bruit' : \n",
    "\n",
    "        for i in abscisse:\n",
    "\n",
    "            sharpe_os.append(prediction_sharpe(modele, nb_simus, nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,i,rank_betas).iloc[0,1])\n",
    "            sharpe_theo.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,i,rank_betas).iloc[0,2])\n",
    "            ecart_sharpe.append(prediction_sharpe(modele, nb_simus,nb_signals,nb_dates_in_sample,nb_dates_out_sample,nb_assets,vol_assets,\n",
    "                      correl_assets,vol_signals,correl_signals,i,rank_betas).iloc[0,3])\n",
    "\n",
    "\n",
    "\n",
    "    else: \n",
    "\n",
    "        return (\"pas un paramètre à faire varier fdp\")\n",
    "\n",
    "\n",
    "    plt.plot(abscisse, sharpe_os, label='sharpe out sample')\n",
    "    plt.plot(abscisse, sharpe_theo, label='sharpe théorique optimal')\n",
    "    plt.plot(abscisse, ecart_sharpe, label='écart entre sharpe OS et théorique')\n",
    "\n",
    "    # Ajouter des labels et un titre\n",
    "    plt.xlabel(parametre_en_abscisse)\n",
    "    plt.ylabel('Valeurs des sharpes')\n",
    "    plt.title(f'sharpe moyen pour {nb_simus} simulations avec le modèle {modele}')\n",
    "\n",
    "    # Ajouter une légende\n",
    "    plt.legend()\n",
    "\n",
    "    # Afficher le tracé\n",
    "    plt.show()\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Le temps d'exécution est de {:.2f} secondes.\".format(execution_time))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jolis_graphiques' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mjolis_graphiques\u001b[49m(modele\u001b[38;5;241m=\u001b[39mLinearRegression(fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \n\u001b[0;32m      2\u001b[0m                  parametre_en_abscisse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnombre_dates_in_sample\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m                  min_abscisse\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m260\u001b[39m, \n\u001b[0;32m      4\u001b[0m                  max_abscisse\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m260\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      5\u001b[0m                  pas\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m260\u001b[39m, \n\u001b[0;32m      6\u001b[0m                  nb_simus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m      7\u001b[0m                  nb_signals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      8\u001b[0m                  nb_dates_in_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1300\u001b[39m,\n\u001b[0;32m      9\u001b[0m                  nb_dates_out_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m260\u001b[39m,\n\u001b[0;32m     10\u001b[0m                  nb_assets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m,\n\u001b[0;32m     11\u001b[0m                  vol_assets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     12\u001b[0m                  correl_assets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[0;32m     13\u001b[0m                  vol_signals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     14\u001b[0m                  correl_signals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[0;32m     15\u001b[0m                  signal_to_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m     16\u001b[0m                  rank_betas\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'jolis_graphiques' is not defined"
     ]
    }
   ],
   "source": [
    "jolis_graphiques(modele=LinearRegression(fit_intercept=False), \n",
    "                 parametre_en_abscisse='nombre_dates_in_sample', \n",
    "                 min_abscisse=260, \n",
    "                 max_abscisse=260*20,\n",
    "                 pas=260, \n",
    "                 nb_simus=1000,\n",
    "                 nb_signals=100,\n",
    "                 nb_dates_in_sample=1300,\n",
    "                 nb_dates_out_sample=260,\n",
    "                 nb_assets=80,\n",
    "                 vol_assets=0.2/16,\n",
    "                 correl_assets=0.8,\n",
    "                 vol_signals=0.2/16,\n",
    "                 correl_signals=0.8,\n",
    "                 signal_to_noise=0.001,\n",
    "                 rank_betas=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jolis_graphiques' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mjolis_graphiques\u001b[49m(modele\u001b[38;5;241m=\u001b[39mRidge(fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \n\u001b[0;32m      2\u001b[0m                  parametre_en_abscisse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnombre_dates_in_sample\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m                  min_abscisse\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m260\u001b[39m, \n\u001b[0;32m      4\u001b[0m                  max_abscisse\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m260\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      5\u001b[0m                  pas\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m260\u001b[39m, \n\u001b[0;32m      6\u001b[0m                  nb_simus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m      7\u001b[0m                  nb_signals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      8\u001b[0m                  nb_dates_in_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1300\u001b[39m,\n\u001b[0;32m      9\u001b[0m                  nb_dates_out_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m260\u001b[39m,\n\u001b[0;32m     10\u001b[0m                  nb_assets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m,\n\u001b[0;32m     11\u001b[0m                  vol_assets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     12\u001b[0m                  correl_assets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[0;32m     13\u001b[0m                  vol_signals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     14\u001b[0m                  correl_signals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[0;32m     15\u001b[0m                  signal_to_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m     16\u001b[0m                  rank_betas\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'jolis_graphiques' is not defined"
     ]
    }
   ],
   "source": [
    "jolis_graphiques(modele=Ridge(fit_intercept=False), \n",
    "                 parametre_en_abscisse='nombre_dates_in_sample', \n",
    "                 min_abscisse=260, \n",
    "                 max_abscisse=260*20,\n",
    "                 pas=260, \n",
    "                 nb_simus=1000,\n",
    "                 nb_signals=100,\n",
    "                 nb_dates_in_sample=1300,\n",
    "                 nb_dates_out_sample=260,\n",
    "                 nb_assets=80,\n",
    "                 vol_assets=0.2/16,\n",
    "                 correl_assets=0.8,\n",
    "                 vol_signals=0.2/16,\n",
    "                 correl_signals=0.8,\n",
    "                 signal_to_noise=0.001,\n",
    "                 rank_betas=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idée: ce serait plus pertinent de produire un graphe qui renvoie le ratio sharpe_out_sample/(sharpe_theorique - sharpe_out_sample) associé à différentes méthodes, avec en abscisse le parametre qu'on fait varier.\n",
    "\n",
    "Comme ça on capture à la fois la performance prédictive du modele au numérateur et l'overfitting au dénominateur et on peut comparer les différentes méthodes entre elles "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
