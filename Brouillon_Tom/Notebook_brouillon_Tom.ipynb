{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Modelisation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "#Fonction test model\n",
    "from Test_Model import test_model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III/ Comparaison de l'erreur de prédiction de différentes méthodes statistiques\n",
    "\n",
    "Jusqu'à maintenant, nous avons démontré que les stratégies d'investissements obtenues à partir de la résolution du problème de Markovitz (équivalent à la résolution d'un problème des moindres carrés => démonstration dans le fichier LaTeX) ne sont pas très convaincantes. Nous allons donc tester plusieurs autres méthodes statistiques de prédiction des rendements futurs et allons mettre en place une métrique commune qui mesure l'erreur de prédiction obtenue. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on crée l'interface qui nous permet de choisir le nombre d'actifs considéré, le nombre de dates, la vraie loi des rendements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actifs = 20\n",
    "nb_dates = 256 * 5\n",
    "\n",
    "idx = pd.Index(np.arange(nb_actifs))\n",
    "idx_date = pd.Index(np.arange(nb_dates))\n",
    "\n",
    "esp_rdt = 0.05\n",
    "vol_rdt = 0.2\n",
    "correl = 0.7\n",
    "seuil_pb_marko = 0.05\n",
    "mu = pd.Series(esp_rdt,index=idx)\n",
    "vols = pd.Series(vol_rdt,index=idx)\n",
    "#Le vecteur constant cible que l'on essayer d'estimer -> chaque composante du vecteur est le rendement du protefeuille optimal tous  les jours\n",
    "allocation_optimale_théorique = pd.Series(esp_rdt, index = idx_date)\n",
    "\n",
    "#Matrice var-covar théorique selon la vraie loi des rendements\n",
    "covar = np.diag(vols) @ pd.DataFrame(correl * np.ones((nb_actifs,nb_actifs)) + (1-correl) * np.eye(nb_actifs),index=idx,columns=idx) @ np.diag(vols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On simuler un tirage de rendements suivant une loi normale d'espérence Mu et de Var la matrice Covar\n",
    "A = np.linalg.cholesky(covar)\n",
    "rdts_observes = mu/256 + pd.DataFrame(np.random.randn(nb_dates,nb_actifs)) @ A.T / 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on calcule les estimateurs d'espérance et de variance empiriques de la série des observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_estime = rdts_observes.mean() * 256\n",
    "covar_estimee = rdts_observes.cov() * 256\n",
    "\n",
    "#Suivant la formule de la résolution du problème, on calcule l'allocation optimale \"in sample\"\n",
    "\n",
    "covar_inv = np.linalg.inv(covar_estimee)\n",
    "df_covar_inv = pd.DataFrame(covar_inv,index=idx,columns=idx)\n",
    "lambda1 = seuil_pb_marko/(mu_estime @ df_covar_inv @ mu_estime)\n",
    "alloc_optimale_in_sample = lambda1 * (df_covar_inv @ mu_estime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori, nous pouvons nous ramener à la résolution d'un problème des moindres carrés. Donc nous allons effectuer la régression de la vraie allocation optimale sur les rendements observés. Le Beta que l'on obtient devrait être le plus proche possible de l'allocation optimale théorique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.05\n",
      "1       0.05\n",
      "2       0.05\n",
      "3       0.05\n",
      "4       0.05\n",
      "        ... \n",
      "1275    0.05\n",
      "1276    0.05\n",
      "1277    0.05\n",
      "1278    0.05\n",
      "1279    0.05\n",
      "Length: 1280, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(allocation_optimale_théorique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "\n",
    "# Entraîner le modèle sur les données\n",
    "regression.fit(rdts_observes, allocation_optimale_théorique)\n",
    "\n",
    "# Obtenir les coefficients de la régression\n",
    "coefficients = regression.coef_\n",
    "intercept = regression.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réalise la régression sans intercept car la variable sur laquelle on régresse est colinéaire avec le vecteur constant égale à 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_sans_intercept = LinearRegression(fit_intercept = False)\n",
    "\n",
    "# Entraîner le modèle sur les données\n",
    "regression_sans_intercept.fit(rdts_observes, allocation_optimale_théorique)\n",
    "\n",
    "# Obtenir les coefficients de la régression\n",
    "coefficients_bis = regression_sans_intercept.coef_\n",
    "intercept_bis = regression_sans_intercept.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On normalise le vecteur d'allocation entre les actifs in sample afin que la somme de ses poids soit égale à 1 et pareil pour le vecteur des coefficients qui sort de la régression sans intercept. \n",
    "\n",
    "On voit que l'on retrouve bien l'allocation in sample de la partie I. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    -3.774758e-14\n",
      "1    -3.819167e-14\n",
      "2    -1.121325e-14\n",
      "3    -2.753353e-14\n",
      "4    -4.418688e-14\n",
      "5    -9.858780e-14\n",
      "6     4.307665e-14\n",
      "7     6.927792e-14\n",
      "8    -1.145750e-13\n",
      "9     5.284662e-14\n",
      "10    5.151435e-14\n",
      "11   -1.887379e-15\n",
      "12   -4.396483e-14\n",
      "13    4.551914e-15\n",
      "14   -3.730349e-14\n",
      "15   -4.884981e-14\n",
      "16    7.682743e-14\n",
      "17    7.194245e-14\n",
      "18    1.048051e-13\n",
      "19    3.064216e-14\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Coefficients problème de Markovitz\n",
    "alloc_optimale_in_sample_normalized = alloc_optimale_in_sample * (1 / sum(alloc_optimale_in_sample))\n",
    "#Coefficients issues de la régression\n",
    "coefficients_bis_normalized = coefficients_bis * (1 / sum(coefficients_bis))\n",
    "\n",
    "print(alloc_optimale_in_sample_normalized - coefficients_bis_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/ Régressions pénalisées\n",
    "\n",
    "Dans cette partie, nous allons tester des régressions pénalisées afin de pouvoir comparer l'erreur de prédiction avec les précédentes méthodes\n",
    "\n",
    "### 1/ Création des échantillons train/test\n",
    "\n",
    "Ici, nous allons régresser le vecteur \"allocation_optimale_theorique\" qui est un vecteur colinéaire au vecteur constant égal à 1 et qui contient la performance, jour par jour, du portfolio optimal dans notre exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = rdts_observes\n",
    "Y_train = allocation_optimale_théorique\n",
    "\n",
    "X_test = mu/256 + pd.DataFrame(np.random.randn(nb_dates,nb_actifs)) @ A.T / 16\n",
    "Y_test = allocation_optimale_théorique #Car les lois des actifs n'ont pas été modifiées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_2 du modèle : 0.0\n",
      "Le MSE (Mean Squared Error) de notre prédiction est de 4.81482486096809e-35\n",
      "Le MAE (Mean Absolute Error) de notre prédiction est de 6.938893903907228e-18\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sg/lxw89kss0x727nlhml9vq0nm0000gn/T/ipykernel_56304/267440285.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLassoCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/ENSAE/Projet StatApp/StatApp_M1_ENSAE/Brouillon_Tom/Test_Model.py\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(X_train, X_test, y_train, y_test, DF, model)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#On va voir si la droite des moindres carrés entre les valeurs prédites et les vraies valeurs est proche de la droite x=y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mslope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6206\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model(X_train, X_test, Y_train, Y_test, X_train, LassoCV())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.004011</td>\n",
       "      <td>-0.006507</td>\n",
       "      <td>-0.008014</td>\n",
       "      <td>-0.005071</td>\n",
       "      <td>-0.003630</td>\n",
       "      <td>-0.007668</td>\n",
       "      <td>-0.004176</td>\n",
       "      <td>-0.012745</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>-0.001367</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>-0.005131</td>\n",
       "      <td>-0.004204</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.029332</td>\n",
       "      <td>-0.006098</td>\n",
       "      <td>-0.010613</td>\n",
       "      <td>-0.019406</td>\n",
       "      <td>-0.001325</td>\n",
       "      <td>-0.019010</td>\n",
       "      <td>-0.011383</td>\n",
       "      <td>-0.017337</td>\n",
       "      <td>-0.013397</td>\n",
       "      <td>-0.022939</td>\n",
       "      <td>-0.016286</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>-0.002804</td>\n",
       "      <td>-0.013797</td>\n",
       "      <td>-0.008263</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>-0.012145</td>\n",
       "      <td>-0.022775</td>\n",
       "      <td>-0.027990</td>\n",
       "      <td>-0.005366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.013123</td>\n",
       "      <td>-0.000320</td>\n",
       "      <td>-0.016851</td>\n",
       "      <td>-0.002304</td>\n",
       "      <td>-0.005287</td>\n",
       "      <td>-0.009148</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>-0.024080</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>-0.010963</td>\n",
       "      <td>-0.010186</td>\n",
       "      <td>-0.001193</td>\n",
       "      <td>-0.007121</td>\n",
       "      <td>-0.010610</td>\n",
       "      <td>-0.001295</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-0.011048</td>\n",
       "      <td>-0.008058</td>\n",
       "      <td>-0.004800</td>\n",
       "      <td>-0.012194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004652</td>\n",
       "      <td>0.011380</td>\n",
       "      <td>-0.006680</td>\n",
       "      <td>0.014287</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>0.009048</td>\n",
       "      <td>0.012692</td>\n",
       "      <td>0.016519</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>0.014722</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.020591</td>\n",
       "      <td>0.004118</td>\n",
       "      <td>0.006223</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.005520</td>\n",
       "      <td>-0.000505</td>\n",
       "      <td>-0.000263</td>\n",
       "      <td>-0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003386</td>\n",
       "      <td>-0.001456</td>\n",
       "      <td>0.010897</td>\n",
       "      <td>-0.002985</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.008748</td>\n",
       "      <td>-0.004806</td>\n",
       "      <td>0.009995</td>\n",
       "      <td>0.007336</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.012035</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>0.010561</td>\n",
       "      <td>0.011862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>-0.001190</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>-0.001049</td>\n",
       "      <td>-0.011770</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>-0.006323</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>-0.011983</td>\n",
       "      <td>-0.002045</td>\n",
       "      <td>0.010129</td>\n",
       "      <td>-0.005878</td>\n",
       "      <td>0.007613</td>\n",
       "      <td>-0.005334</td>\n",
       "      <td>-0.009540</td>\n",
       "      <td>0.009312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>-0.001494</td>\n",
       "      <td>0.008845</td>\n",
       "      <td>-0.000767</td>\n",
       "      <td>-0.001391</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>-0.004457</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.007902</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>0.010252</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.013542</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.010899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>0.021684</td>\n",
       "      <td>0.020739</td>\n",
       "      <td>0.009811</td>\n",
       "      <td>0.014797</td>\n",
       "      <td>0.020185</td>\n",
       "      <td>0.009602</td>\n",
       "      <td>0.024353</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.015629</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.016427</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.016604</td>\n",
       "      <td>0.005127</td>\n",
       "      <td>0.018039</td>\n",
       "      <td>0.017239</td>\n",
       "      <td>0.033714</td>\n",
       "      <td>0.021533</td>\n",
       "      <td>0.011194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>-0.019375</td>\n",
       "      <td>-0.012638</td>\n",
       "      <td>-0.016942</td>\n",
       "      <td>-0.013082</td>\n",
       "      <td>-0.015119</td>\n",
       "      <td>-0.015928</td>\n",
       "      <td>-0.007209</td>\n",
       "      <td>-0.016882</td>\n",
       "      <td>-0.017976</td>\n",
       "      <td>-0.005162</td>\n",
       "      <td>-0.005515</td>\n",
       "      <td>-0.021445</td>\n",
       "      <td>-0.017442</td>\n",
       "      <td>-0.015287</td>\n",
       "      <td>-0.013872</td>\n",
       "      <td>-0.014736</td>\n",
       "      <td>-0.023975</td>\n",
       "      <td>-0.019911</td>\n",
       "      <td>-0.012303</td>\n",
       "      <td>-0.022733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>-0.025811</td>\n",
       "      <td>-0.029051</td>\n",
       "      <td>-0.020668</td>\n",
       "      <td>-0.024090</td>\n",
       "      <td>-0.020444</td>\n",
       "      <td>-0.026875</td>\n",
       "      <td>-0.022595</td>\n",
       "      <td>-0.024711</td>\n",
       "      <td>-0.021811</td>\n",
       "      <td>-0.021315</td>\n",
       "      <td>-0.032200</td>\n",
       "      <td>-0.026825</td>\n",
       "      <td>-0.022063</td>\n",
       "      <td>-0.009181</td>\n",
       "      <td>-0.030262</td>\n",
       "      <td>-0.024660</td>\n",
       "      <td>-0.020690</td>\n",
       "      <td>-0.020187</td>\n",
       "      <td>-0.021950</td>\n",
       "      <td>-0.016226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.004011 -0.006507 -0.008014 -0.005071 -0.003630 -0.007668 -0.004176   \n",
       "1    -0.029332 -0.006098 -0.010613 -0.019406 -0.001325 -0.019010 -0.011383   \n",
       "2    -0.013123 -0.000320 -0.016851 -0.002304 -0.005287 -0.009148  0.002058   \n",
       "3     0.004652  0.011380 -0.006680  0.014287  0.009830  0.009048  0.012692   \n",
       "4     0.003386 -0.001456  0.010897 -0.002985  0.003557  0.002550  0.008748   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1275  0.004347  0.001190 -0.001190  0.007826  0.004810 -0.004741 -0.001049   \n",
       "1276 -0.001494  0.008845 -0.000767 -0.001391  0.003421  0.001082  0.001187   \n",
       "1277  0.021684  0.020739  0.009811  0.014797  0.020185  0.009602  0.024353   \n",
       "1278 -0.019375 -0.012638 -0.016942 -0.013082 -0.015119 -0.015928 -0.007209   \n",
       "1279 -0.025811 -0.029051 -0.020668 -0.024090 -0.020444 -0.026875 -0.022595   \n",
       "\n",
       "            7         8         9         10        11        12        13  \\\n",
       "0    -0.012745  0.000866 -0.001367 -0.000713  0.004700  0.000345  0.005222   \n",
       "1    -0.017337 -0.013397 -0.022939 -0.016286  0.003045 -0.002804 -0.013797   \n",
       "2    -0.024080  0.000432 -0.010963 -0.010186 -0.001193 -0.007121 -0.010610   \n",
       "3     0.016519  0.001373  0.018280  0.014722  0.005099  0.020591  0.004118   \n",
       "4    -0.004806  0.009995  0.007336  0.003410  0.012035  0.003713 -0.000140   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1275 -0.011770  0.001982  0.001959 -0.006323  0.007129 -0.011983 -0.002045   \n",
       "1276 -0.004457  0.002903  0.001984  0.004237  0.007902  0.009360  0.010252   \n",
       "1277  0.018817  0.015629  0.011164  0.016427  0.001249  0.022068  0.016604   \n",
       "1278 -0.016882 -0.017976 -0.005162 -0.005515 -0.021445 -0.017442 -0.015287   \n",
       "1279 -0.024711 -0.021811 -0.021315 -0.032200 -0.026825 -0.022063 -0.009181   \n",
       "\n",
       "            14        15        16        17        18        19  \n",
       "0     0.006773  0.001946  0.002605 -0.005131 -0.004204  0.000065  \n",
       "1    -0.008263 -0.008861 -0.012145 -0.022775 -0.027990 -0.005366  \n",
       "2    -0.001295 -0.007413 -0.011048 -0.008058 -0.004800 -0.012194  \n",
       "3     0.006223  0.015918  0.005520 -0.000505 -0.000263 -0.000340  \n",
       "4     0.005767  0.002755  0.009410  0.005743  0.010561  0.011862  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "1275  0.010129 -0.005878  0.007613 -0.005334 -0.009540  0.009312  \n",
       "1276  0.000723  0.013542  0.006076  0.002470  0.001906  0.010899  \n",
       "1277  0.005127  0.018039  0.017239  0.033714  0.021533  0.011194  \n",
       "1278 -0.013872 -0.014736 -0.023975 -0.019911 -0.012303 -0.022733  \n",
       "1279 -0.030262 -0.024660 -0.020690 -0.020187 -0.021950 -0.016226  \n",
       "\n",
       "[1280 rows x 20 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
